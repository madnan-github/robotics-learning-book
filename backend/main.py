"""
Embedding Pipeline for Docusaurus Content

This module implements a pipeline that:
1. Crawls Docusaurus URLs to extract all accessible pages
2. Extracts clean text content from each page
3. Chunks the text appropriately for embedding generation
4. Generates embeddings using Cohere
5. Stores embeddings with metadata in Qdrant
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path
import hashlib
import time
import os
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
import cohere
from qdrant_client import QdrantClient
from qdrant_client.http import models
from dotenv import load_dotenv


# Load environment variables
load_dotenv()


def load_environment_variables() -> Tuple[str, str, Optional[str]]:
    """Load and validate environment variables."""
    cohere_api_key = os.getenv("COHERE_API_KEY")
    qdrant_url = os.getenv("QDRANT_URL")
    qdrant_api_key = os.getenv("QDRANT_API_KEY")  # Optional for local Qdrant

    if not cohere_api_key:
        raise ValueError("COHERE_API_KEY environment variable is required")
    if not qdrant_url:
        raise ValueError("QDRANT_URL environment variable is required")

    return cohere_api_key, qdrant_url, qdrant_api_key


# Initialize Cohere client
def initialize_cohere_client() -> cohere.Client:
    """Initialize Cohere client with proper error handling."""
    try:
        cohere_api_key, _, _ = load_environment_variables()
        client = cohere.Client(api_key=cohere_api_key)
        logger.info("Cohere client initialized successfully")
        return client
    except Exception as e:
        logger.error(f"Failed to initialize Cohere client: {e}")
        raise


# Initialize Qdrant client
def initialize_qdrant_client() -> QdrantClient:
    """Initialize Qdrant client with proper configuration."""
    try:
        _, qdrant_url, qdrant_api_key = load_environment_variables()
        client = QdrantClient(
            url=qdrant_url,
            api_key=qdrant_api_key,
            prefer_grpc=False  # Using HTTP for better compatibility
        )
        logger.info("Qdrant client initialized successfully")
        return client
    except Exception as e:
        logger.error(f"Failed to initialize Qdrant client: {e}")
        raise


# Create Qdrant collection for embeddings
def create_qdrant_collection(client: QdrantClient, collection_name: str = "embeddings") -> None:
    """Create Qdrant collection for embeddings with proper schema (vector_size: 1024, distance: cosine)."""
    try:
        # Check if collection already exists
        collections = client.get_collections().collections
        collection_exists = any(col.name == collection_name for col in collections)

        if not collection_exists:
            client.create_collection(
                collection_name=collection_name,
                vectors_config=models.VectorParams(
                    size=1024,  # Cohere embedding dimension
                    distance=models.Distance.COSINE
                )
            )
            logger.info(f"Qdrant collection '{collection_name}' created successfully")
        else:
            logger.info(f"Qdrant collection '{collection_name}' already exists")
    except Exception as e:
        logger.error(f"Failed to create Qdrant collection: {e}")
        raise


def rate_limit_pause():
    """Add rate limiting pause to handle API rate limits."""
    time.sleep(1)  # Simple rate limiting - pause for 1 second


@dataclass
class Document:
    """Represents extracted text content from a single URL with metadata."""
    url: str
    title: str
    content: str
    extraction_timestamp: datetime
    content_hash: str


@dataclass
class Chunk:
    """Represents a segment of text content that will be converted to an embedding."""
    chunk_id: str
    document_url: str
    content: str
    chunk_index: int
    metadata: Dict
    embedding_vector: Optional[List[float]] = None


@dataclass
class Embedding:
    """Vector representation of text content generated by Cohere with associated metadata."""
    vector_id: str
    vector: List[float]
    document_url: str
    chunk_id: str
    metadata: Dict


def get_all_urls(base_url: str) -> List[str]:
    """
    Crawl Docusaurus site and extract all accessible URLs from the target site.

    Args:
        base_url: The base URL of the Docusaurus site to crawl

    Returns:
        List of all accessible URLs found on the site
    """
    urls = set()
    urls.add(base_url)

    try:
        # Get the main page to find links
        response = requests.get(base_url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Find all links on the page
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)

            # Only add URLs from the same domain and that are likely documentation pages
            if urlparse(full_url).netloc == urlparse(base_url).netloc:
                if full_url.endswith(('.html', '/')) and '#' not in full_url:
                    urls.add(full_url)

        # For a more thorough crawl, we could recursively follow links
        # But for now, let's just get the top-level links
        logger.info(f"Found {len(urls)} URLs from {base_url}")
        return list(urls)

    except Exception as e:
        logger.error(f"Error crawling {base_url}: {e}")
        return [base_url]  # Return the base URL as fallback


def extract_text_from_url(url: str) -> Tuple[str, str]:
    """
    Extract clean text content from a single Docusaurus page.

    Args:
        url: URL of the page to extract text from

    Returns:
        Tuple of (title, content) with the page title and cleaned text content
    """
    try:
        response = requests.get(url)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()

        # Try to find the main content area in Docusaurus sites
        # Common selectors for Docusaurus content areas
        content_selectors = [
            'main div[class*="docItemContainer"]',  # Docusaurus v2+
            'article',  # Standard article tag
            'main div[class*="container"]',  # Docusaurus container
            'div[class*="doc"]',  # Docusaurus doc classes
            'div[class*="theme"]',  # Docusaurus theme content
            'main'  # Main content area
        ]

        content_element = None
        for selector in content_selectors:
            content_element = soup.select_one(selector)
            if content_element:
                break

        # If no specific content area found, use the body
        if not content_element:
            content_element = soup.find('body')

        # Extract text from the content area
        if content_element:
            # Get text and clean it up
            content = content_element.get_text(separator=' ')
        else:
            # Fallback: get all text from the page
            content = soup.get_text(separator=' ')

        # Clean up the text
        lines = (line.strip() for line in content.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        content = ' '.join(chunk for chunk in chunks if chunk)

        # Get the title
        title_tag = soup.find('title')
        title = title_tag.get_text().strip() if title_tag else urlparse(url).path.split('/')[-1] or 'Untitled'

        logger.info(f"Successfully extracted content from {url} (title: {title})")
        return title, content

    except Exception as e:
        logger.error(f"Error extracting text from {url}: {e}")
        return "Error", f"Error extracting content from {url}: {str(e)}"


def is_valid_docusaurus_url(url: str) -> bool:
    """
    Validate if a URL is a valid Docusaurus page.

    Args:
        url: URL to validate

    Returns:
        True if the URL is valid, False otherwise
    """
    try:
        response = requests.head(url, timeout=10)
        # Check if it's an HTML page
        content_type = response.headers.get('content-type', '').lower()
        return 'text/html' in content_type
    except:
        return False


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 100) -> List[Dict[str, any]]:
    """
    Split text into appropriate sizes for embedding generation.

    Args:
        text: The text to be chunked
        chunk_size: Maximum size of each chunk (in characters)
        overlap: Number of overlapping characters between chunks

    Returns:
        List of chunks with content and metadata
    """
    if not text:
        return []

    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        end = start + chunk_size

        # If this is the last chunk and it's smaller than chunk_size, include it all
        if end > text_length:
            end = text_length

        chunk_content = text[start:end]
        chunk_data = {
            'content': chunk_content,
            'start_pos': start,
            'end_pos': end
        }

        chunks.append(chunk_data)

        # Move start position by chunk_size minus overlap
        start = end - overlap

        # If the remaining text is less than overlap, we're done
        if text_length - end < overlap:
            break

    logger.info(f"Text chunked into {len(chunks)} chunks")
    return chunks


def embed(text: str, cohere_client: cohere.Client, model: str = "embed-english-v3.0") -> List[float]:
    """
    Generate embeddings using Cohere API with proper error handling.

    Args:
        text: The text to embed
        cohere_client: Initialized Cohere client
        model: The embedding model to use

    Returns:
        The embedding vector as a list of floats
    """
    try:
        # Cohere has a limit on text length, so we might need to handle longer texts
        # For now, we'll just embed the text directly
        response = cohere_client.embed(
            texts=[text],
            model=model,
            input_type="search_document"  # Using search_document as default input type
        )

        if response.embeddings and len(response.embeddings) > 0:
            embedding = response.embeddings[0]
            logger.info(f"Successfully generated embedding with {len(embedding)} dimensions")
            return embedding
        else:
            raise ValueError("No embeddings returned from Cohere API")

    except Exception as e:
        logger.error(f"Error generating embedding for text: {e}")
        raise


def validate_embedding_dimensions(embedding: List[float], expected_size: int = 1024) -> bool:
    """
    Validate that the embedding has the correct dimensions.

    Args:
        embedding: The embedding vector to validate
        expected_size: Expected dimension size

    Returns:
        True if valid, False otherwise
    """
    if not isinstance(embedding, list):
        return False
    if len(embedding) != expected_size:
        logger.warning(f"Embedding has {len(embedding)} dimensions, expected {expected_size}")
        return False
    # Check if all values are finite numbers
    for value in embedding:
        if not isinstance(value, (int, float)) or not (float('-inf') < value < float('inf')):
            return False
    return True


def generate_embedding_cache_key(text: str) -> str:
    """
    Generate a cache key for a text to avoid re-generating embeddings.

    Args:
        text: The text to generate cache key for

    Returns:
        A hash string to use as cache key
    """
    return hashlib.md5(text.encode('utf-8')).hexdigest()


def save_chunk_to_qdrant(
    chunk_data: Dict,
    vector: List[float],
    metadata: Dict,
    qdrant_client: QdrantClient,
    collection_name: str = "embeddings"
) -> str:
    """
    Store embeddings in Qdrant with proper metadata.

    Args:
        chunk_data: The original chunk data
        vector: The embedding vector
        metadata: Additional metadata to store with the embedding
        qdrant_client: Initialized Qdrant client
        collection_name: Name of the Qdrant collection

    Returns:
        The ID of the stored vector in Qdrant
    """
    try:
        # Generate a unique ID for this vector as a UUID hex string
        content_to_hash = f"{metadata.get('url', 'unknown')}_{str(chunk_data)}_{datetime.now().isoformat()}"
        vector_id = hashlib.md5(content_to_hash.encode()).hexdigest()

        # Prepare the payload with metadata
        payload = {
            "url": metadata.get("url", ""),
            "title": metadata.get("title", ""),
            "chunk_index": metadata.get("chunk_index", 0),
            "content_preview": metadata.get("content", "")[:200],  # First 200 chars as preview
            "created_at": int(datetime.now().timestamp()),
            **metadata  # Include any additional metadata
        }

        # Upsert the vector to Qdrant
        qdrant_client.upsert(
            collection_name=collection_name,
            points=[
                models.PointStruct(
                    id=vector_id,
                    vector=vector,
                    payload=payload
                )
            ]
        )

        logger.info(f"Successfully stored embedding to Qdrant with ID: {vector_id}")
        return vector_id

    except Exception as e:
        logger.error(f"Error storing embedding to Qdrant: {e}")
        raise


def search_similar_embeddings(
    query_vector: List[float],
    qdrant_client: QdrantClient,
    collection_name: str = "embeddings",
    limit: int = 10
) -> List[Dict]:
    """
    Retrieve relevant embeddings from Qdrant using similarity search.

    Args:
        query_vector: The embedding vector to search for similar ones
        qdrant_client: Initialized Qdrant client
        collection_name: Name of the Qdrant collection
        limit: Maximum number of results to return

    Returns:
        List of similar embeddings with their metadata
    """
    try:
        results = qdrant_client.search(
            collection_name=collection_name,
            query_vector=query_vector,
            limit=limit
        )

        # Format results to return
        formatted_results = []
        for result in results:
            formatted_results.append({
                "id": result.id,
                "score": result.score,
                "payload": result.payload,
                "vector": result.vector
            })

        logger.info(f"Found {len(formatted_results)} similar embeddings")
        return formatted_results

    except Exception as e:
        logger.error(f"Error searching similar embeddings: {e}")
        raise


def check_duplicate_content(
    content: str,
    qdrant_client: QdrantClient,
    collection_name: str = "embeddings",
    threshold: float = 0.95
) -> bool:
    """
    Check if content already exists in Qdrant to avoid storing duplicates.

    Args:
        content: The content to check for duplicates
        qdrant_client: Initialized Qdrant client
        collection_name: Name of the Qdrant collection
        threshold: Similarity threshold above which content is considered duplicate

    Returns:
        True if duplicate is found, False otherwise
    """
    try:
        # Create a simple hash of the content to use for initial check
        content_hash = hashlib.md5(content.encode()).hexdigest()

        # In a real implementation, we would search for similar embeddings
        # For now, we'll just return False to indicate no duplicate found
        # This is a simplified version - a full implementation would compare
        # the embedding of the content with existing embeddings
        logger.info(f"Duplicate check completed for content with hash: {content_hash}")
        return False

    except Exception as e:
        logger.error(f"Error checking for duplicate content: {e}")
        return False


def rag_embedding(
    target_url: str = "https://robotics-learning-book79.vercel.app/",
    chunk_size: int = 1000,
    overlap: int = 100
) -> None:
    """
    Orchestrate the complete pipeline flow.

    Args:
        target_url: The URL of the Docusaurus site to process
        chunk_size: Size of text chunks for embedding
        overlap: Overlap between chunks
    """
    logger.info(f"Starting RAG embedding pipeline for {target_url}")

    # Initialize clients
    cohere_client = initialize_cohere_client()
    qdrant_client = initialize_qdrant_client()

    # Create Qdrant collection if it doesn't exist
    create_qdrant_collection(qdrant_client)

    # Get all URLs from the target site
    urls = get_all_urls(target_url)
    logger.info(f"Processing {len(urls)} URLs from {target_url}")

    processed_count = 0
    for i, url in enumerate(urls):
        try:
            logger.info(f"Processing URL {i+1}/{len(urls)}: {url}")

            # Extract text from the URL
            title, content = extract_text_from_url(url)
            if not content or content.startswith("Error"):
                logger.warning(f"Failed to extract content from {url}, skipping")
                continue

            # Chunk the content
            chunks = chunk_text(content, chunk_size=chunk_size, overlap=overlap)

            # Process each chunk
            for j, chunk in enumerate(chunks):
                try:
                    # Embed the chunk
                    embedding_vector = embed(chunk['content'], cohere_client)

                    # Validate embedding dimensions
                    if not validate_embedding_dimensions(embedding_vector):
                        logger.warning(f"Invalid embedding dimensions for chunk {j} of {url}")
                        continue

                    # Prepare metadata
                    metadata = {
                        "url": url,
                        "title": title,
                        "chunk_index": j,
                        "content": chunk['content'][:200],  # Store first 200 chars as content preview
                        "source_position": f"{chunk['start_pos']}-{chunk['end_pos']}"
                    }

                    # Check for duplicates before storing
                    is_duplicate = check_duplicate_content(chunk['content'], qdrant_client)
                    if is_duplicate:
                        logger.info(f"Duplicate content found for chunk {j} of {url}, skipping")
                        continue

                    # Save to Qdrant
                    vector_id = save_chunk_to_qdrant(
                        chunk_data=chunk,
                        vector=embedding_vector,
                        metadata=metadata,
                        qdrant_client=qdrant_client
                    )

                    logger.info(f"Stored chunk {j} of {url} with vector ID: {vector_id}")

                    # Simple rate limiting to avoid overwhelming APIs
                    rate_limit_pause()

                except Exception as e:
                    logger.error(f"Error processing chunk {j} of {url}: {e}")
                    continue

            processed_count += 1
            logger.info(f"Completed processing {url} ({processed_count}/{len(urls)})")

        except Exception as e:
            logger.error(f"Error processing URL {url}: {e}")
            continue

    logger.info(f"RAG embedding pipeline completed. Processed {processed_count}/{len(urls)} URLs successfully")


import signal
import sys


def graceful_shutdown(signum, frame):
    """Handle graceful shutdown of the application."""
    logger.info(f"Received signal {signum}, initiating graceful shutdown...")
    # Perform any necessary cleanup here
    logger.info("Cleanup completed. Exiting...")
    sys.exit(0)


def main():
    """Main function with command-line interface for pipeline execution."""
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, graceful_shutdown)
    signal.signal(signal.SIGTERM, graceful_shutdown)

    import argparse

    parser = argparse.ArgumentParser(description="Embedding Pipeline for Docusaurus Content")
    parser.add_argument(
        "--url",
        type=str,
        default="https://robotics-learning-book79.vercel.app/",
        help="Target Docusaurus URL to process (default: https://robotics-learning-book79.vercel.app/)"
    )
    parser.add_argument(
        "--chunk-size",
        type=int,
        default=1000,
        help="Size of text chunks for embedding (default: 1000)"
    )
    parser.add_argument(
        "--overlap",
        type=int,
        default=100,
        help="Overlap between chunks (default: 100)"
    )

    args = parser.parse_args()

    logger.info("Starting embedding pipeline...")
    logger.info(f"Target URL: {args.url}")
    logger.info(f"Chunk size: {args.chunk_size}")
    logger.info(f"Overlap: {args.overlap}")

    try:
        rag_embedding(
            target_url=args.url,
            chunk_size=args.chunk_size,
            overlap=args.overlap
        )
        logger.info("Pipeline completed successfully!")
    except KeyboardInterrupt:
        logger.info("Pipeline interrupted by user")
        graceful_shutdown(signal.SIGINT, None)
    except Exception as e:
        logger.error(f"Pipeline failed with error: {e}")
        raise


# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# Only run the main function if this script is executed directly
if __name__ == "__main__":
    main()