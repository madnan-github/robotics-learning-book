---
sidebar_position: 2
title: "Voice-to-Action Pipeline"
---

# Voice-to-Action Pipeline

This section covers implementing voice command processing using OpenAI Whisper and converting speech to ROS 2 messages that control the robot.

## Whisper Integration Setup

Setting up Whisper for voice processing:

```python
# Install required packages
# pip install openai-whisper ros2

import whisper
import rospy
from std_msgs.msg import String
from geometry_msgs.msg import Twist

class VoiceToActionNode:
    def __init__(self):
        rospy.init_node('voice_to_action')

        # Initialize Whisper model
        self.model = whisper.load_model("base")  # or "small", "medium", "large"

        # Publisher for robot commands
        self.cmd_vel_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)

        # Publisher for recognized text
        self.text_pub = rospy.Publisher('/recognized_text', String, queue_size=10)

        # Audio input subscription (from microphone node)
        self.audio_sub = rospy.Subscriber('/audio_input', AudioData, self.audio_callback)

        rospy.loginfo("Voice to Action node initialized")

    def audio_callback(self, audio_data):
        # Process audio data and convert to text
        try:
            # Convert audio data to appropriate format for Whisper
            audio_array = self.process_audio_data(audio_data)

            # Transcribe audio to text
            result = self.model.transcribe(audio_array)
            recognized_text = result["text"]

            # Publish recognized text
            text_msg = String()
            text_msg.data = recognized_text
            self.text_pub.publish(text_msg)

            # Convert text to robot command
            self.process_command(recognized_text)

        except Exception as e:
            rospy.logerr(f"Error processing audio: {e}")

    def process_audio_data(self, audio_data):
        # Convert ROS audio data to format expected by Whisper
        # This is a simplified example - actual implementation may vary
        import numpy as np
        audio_array = np.frombuffer(audio_data.data, dtype=np.int16)
        # Normalize and convert to float
        audio_array = audio_array.astype(np.float32) / 32768.0
        return audio_array

    def process_command(self, text):
        # Simple command mapping - in practice, use LLM for complex parsing
        text = text.lower().strip()

        cmd_vel = Twist()

        if "move forward" in text or "go forward" in text:
            cmd_vel.linear.x = 0.5
        elif "move backward" in text or "go back" in text:
            cmd_vel.linear.x = -0.5
        elif "turn left" in text:
            cmd_vel.angular.z = 0.5
        elif "turn right" in text:
            cmd_vel.angular.z = -0.5
        elif "stop" in text:
            cmd_vel.linear.x = 0.0
            cmd_vel.angular.z = 0.0
        else:
            rospy.loginfo(f"Unrecognized command: {text}")
            return

        self.cmd_vel_pub.publish(cmd_vel)
        rospy.loginfo(f"Executed command: {text}")

def main():
    try:
        node = VoiceToActionNode()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass

if __name__ == '__main__':
    main()
```

## Audio Processing Pipeline

For more robust voice processing, consider:

```python
import pyaudio
import wave
import threading
from queue import Queue

class AudioProcessor:
    def __init__(self):
        self.audio_queue = Queue()
        self.is_listening = False

        # Audio parameters
        self.format = pyaudio.paInt16
        self.channels = 1
        self.rate = 16000  # Whisper works well at 16kHz
        self.chunk = 1024

        self.audio = pyaudio.PyAudio()

    def start_listening(self):
        self.is_listening = True
        stream = self.audio.open(
            format=self.format,
            channels=self.channels,
            rate=self.rate,
            input=True,
            frames_per_buffer=self.chunk
        )

        def audio_thread():
            while self.is_listening:
                data = stream.read(self.chunk)
                self.audio_queue.put(data)

        threading.Thread(target=audio_thread, daemon=True).start()

    def get_audio_data(self):
        if not self.audio_queue.empty():
            return self.audio_queue.get()
        return None
```

## Voice Command Recognition

Creating a more sophisticated command recognition system:

```python
import re
from typing import Dict, List

class CommandRecognizer:
    def __init__(self):
        self.command_patterns = {
            'move': [
                r'move\s+(forward|backward|ahead)',
                r'go\s+(forward|backward|ahead)',
                r'go\s+to\s+(?P<location>\w+)',
                r'approach\s+(?P<object>\w+)'
            ],
            'rotate': [
                r'turn\s+(left|right)',
                r'rotate\s+(left|right)',
                r'pivot\s+(left|right)'
            ],
            'grasp': [
                r'pick\s+up\s+(?P<object>\w+)',
                r'grasp\s+(?P<object>\w+)',
                r'grab\s+(?P<object>\w+)'
            ],
            'navigate': [
                r'go\s+to\s+(?P<location>\w+)',
                r'head\s+to\s+(?P<location>\w+)',
                r'navigate\s+to\s+(?P<location>\w+)'
            ]
        }

    def recognize_command(self, text: str) -> Dict:
        text = text.lower()
        for cmd_type, patterns in self.command_patterns.items():
            for pattern in patterns:
                match = re.search(pattern, text)
                if match:
                    result = {'type': cmd_type}
                    result.update(match.groupdict())
                    return result
        return {'type': 'unknown', 'text': text}
```

## Integration with ROS 2

For ROS 2 integration, use rclpy:

```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from sensor_msgs.msg import AudioData
from geometry_msgs.msg import Twist

class VoiceToActionROS2(Node):
    def __init__(self):
        super().__init__('voice_to_action')

        # Publisher for robot commands
        self.cmd_vel_pub = self.create_publisher(Twist, '/cmd_vel', 10)

        # Publisher for recognized text
        self.text_pub = self.create_publisher(String, '/recognized_text', 10)

        # Subscription to audio input
        self.audio_sub = self.create_subscription(
            AudioData, '/audio_input', self.audio_callback, 10
        )

        # Initialize Whisper model
        import whisper
        self.model = whisper.load_model("base")

        self.get_logger().info("Voice to Action node initialized")

    def audio_callback(self, audio_data):
        # Process audio and convert to text using Whisper
        # Then convert text to robot commands
        pass
```

## Best Practices

1. **Audio Quality**: Use good quality microphones and reduce background noise
2. **Model Selection**: Balance between model size and inference speed
3. **Error Handling**: Implement fallbacks for recognition failures
4. **Privacy**: Consider privacy implications of voice processing
5. **Localization**: Support for different languages and accents